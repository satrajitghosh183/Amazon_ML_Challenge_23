# -*- coding: utf-8 -*-
"""Product_Length.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1viFaHc_IJcw7EHhFK6pj9heqkooZVhSu
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
import csv
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import chi2
import re
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix

from google.colab import drive
drive.mount('/content/drive')

#test file
path = "/content/drive/MyDrive/dataset/test.csv"
df_test = pd.read_csv(path, escapechar="\\", quoting=csv.QUOTE_NONE, error_bad_lines=False)

# df_test = pd.read_csv(path, escapechar = "\\", quoting = csv.QUOTE_NONE)
df_test.head()

#train file
path = "/content/drive/MyDrive/dataset/train.csv"
df_train = pd.read_csv(path, escapechar="\\", quoting=csv.QUOTE_NONE, error_bad_lines=False)

# df_train = pd.read_csv(path, escapechar = "\\", quoting = csv.QUOTE_NONE)
df_train = df_train.dropna()
df_train.head()

punctuation_signs = list("?:!.,;")
nltk.download('punkt')
nltk.download('wordnet')
wordnet_lemmatizer = WordNetLemmatizer()
nltk.download('stopwords')
stop_words = list(stopwords.words('english'))

df_train['TITLE'] = df_train['TITLE'].str.replace("\r", " ")
df_train['TITLE'] = df_train['TITLE'].str.replace("\n", " ")
df_train['TITLE'] = df_train['TITLE'].str.replace("    ", " ")
df_train['TITLE'] = df_train['TITLE'].str.replace('"', '')
df_train['TITLE'] = df_train['TITLE'].str.lower()
for punct_sign in punctuation_signs:
  df_train['TITLE'] = df_train['TITLE'].str.replace(punct_sign, '')
df_train['TITLE'] = df_train['TITLE'].str.replace("'s", "")

final_cols = ["TITLE", "PRODUCT_LENGTH"]
df_train = df_train[final_cols]
df_train = df_train.iloc[:35000, :]

df_test['TITLE'] = df_test['TITLE'].str.replace("\r", " ")
df_test['TITLE'] = df_test['TITLE'].str.replace("\n", " ")
df_test['TITLE'] = df_test['TITLE'].str.replace("    ", " ")
df_test['TITLE'] = df_test['TITLE'].str.replace('"', '')
df_test['TITLE'] = df_test['TITLE'].str.lower()
for punct_sign in punctuation_signs:
  df_test['TITLE'] = df_test['TITLE'].str.replace(punct_sign, '')
df_test['TITLE'] = df_test['TITLE'].str.replace("'s", "")

final_cols = ["TITLE", "PRODUCT_ID"]
df_test = df_test[final_cols]

df_train.head()

df_test.head()

df_train.isna().sum()

df_test["TITLE"].fillna("No Data", inplace = True)

df_test.isna().sum()

X_train, X_test, y_train = df_train["TITLE"], df_test["TITLE"], df_train["PRODUCT_LENGTH"]

tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=5)
print('1')
X_train_vectors_tfidf = tfidf.fit_transform(X_train)
print(X_train_vectors_tfidf.shape)
print('1')
X_test_vectors_tfidf = tfidf.transform(X_test)
print(X_test_vectors_tfidf.shape)

import numpy as np

# convert y_train to a list of strings
y_train_str = y_train.astype(str).tolist()

# check the unique values of y_train_str
print(np.unique(y_train_str))

lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')
lr_tfidf.fit(X_train_vectors_tfidf, y_train_str)

y_predict = lr_tfidf.predict(X_test_vectors_tfidf)

y_predict

len(y_predict)

d = {"PRODUCT_LENGTH" : y_predict}

df1 = pd.DataFrame(data=d)
df1

df_new = pd.concat([df_test, df1], axis = 1)
df_new.head()

l = ["PRODUCT_ID", "PRODUCT_LENGTH"]
df_new = df_new[l]
df_new

df_new.to_csv("/content/drive/MyDrive/submission.csv", index = False, header = True)

path = "/content/drive/MyDrive/submission.csv"
df_sub = pd.read_csv(path)
df_sub.head()

df_sub.isna().sum()

df_sub.describe()